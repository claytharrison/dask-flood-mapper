{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Flood mapping with dynamic harmonic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import hvplot.xarray  # noqa\n",
    "import numpy as np\n",
    "import pystac_client\n",
    "import xarray as xr\n",
    "from dask.distributed import Client, wait\n",
    "from odc import stac as odc_stac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac516eb",
   "metadata": {},
   "source": [
    "In notebook 3, we could quickly get flooded extents because we had a precomputed set of harmonic parameters available to us. However, if this were not available (e.g. flood mapping on a different SAR product), we would need to compute those on-the-fly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "The workflow looks largely the same, starting with spinning up a Dask client and setting up chunking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=\"12GB\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = {\"time\": 1, \"latitude\": 1300, \"longitude\": 1300}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Cube Definitions\n",
    "\n",
    "The following generic specifications are used for presenting the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinate Reference System - World Geodetic System 1984 (WGS84) in this case\n",
    "crs = \"EPSG:4326\"\n",
    "res = 0.00018  # 20 meter in degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Northern Germany Flood\n",
    "\n",
    "We will again use the study area around Zingst, Germany during Storm Babet  [Wikipedia](https://en.wikipedia.org/wiki/Storm_Babet).\n",
    "\n",
    "However, this time we will need a lot of extra data. At TU Wien we typically use about three years of data to generate robust harmonic parameters. To make sure we have the most up-to-date parameters possible, we'll get Sentinel-1 SIG0 data from the EODC STAC Catalogue for the three years prior to Storm Babet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_time_range = \"2020-10-25/2023-10-25\"\n",
    "flood_time_range = \"2023-10-11/2023-10-25\"\n",
    "minlon, maxlon = 12.3, 13.1\n",
    "minlat, maxlat = 54.3, 54.6\n",
    "bounding_box = [minlon, minlat, maxlon, maxlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "eodc_catalog = pystac_client.Client.open(\"https://stac.eodc.eu/api/v1\")\n",
    "search = eodc_catalog.search(\n",
    "    collections=\"SENTINEL1_SIG0_20M\",\n",
    "    bbox=bounding_box,\n",
    "    datetime=parameters_time_range,\n",
    ")\n",
    "\n",
    "items_sig0 = search.item_collection()\n",
    "items_sig0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc03ca5",
   "metadata": {},
   "source": [
    "Define helper functions as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_orbit_names(items):\n",
    "    return np.array(\n",
    "        [\n",
    "            items[i].properties[\"sat:orbit_state\"][0].upper()\n",
    "            + str(items[i].properties[\"sat:relative_orbit\"])\n",
    "            for i in range(len(items))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def post_process_eodc_cube(dc: xr.Dataset, items, bands):\n",
    "    if not isinstance(bands, tuple):\n",
    "        bands = tuple([bands])\n",
    "    for i in bands:\n",
    "        dc[i] = post_process_eodc_cube_(\n",
    "            dc[i], items, i\n",
    "        )  # https://github.com/TUW-GEO/dask-flood-mapper.git\n",
    "    return dc\n",
    "\n",
    "\n",
    "def post_process_eodc_cube_(dc: xr.Dataset, items, band):\n",
    "    scale = items[0].assets[band].extra_fields.get(\"raster:bands\")[0][\"scale\"]\n",
    "    nodata = items[0].assets[band].extra_fields.get(\"raster:bands\")[0][\"nodata\"]\n",
    "    return dc.where(dc != nodata) / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Lazily load data for VV polarization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = \"VV\"\n",
    "sig0_dc = odc_stac.load(\n",
    "    items_sig0,\n",
    "    bands=bands,\n",
    "    crs=crs,\n",
    "    chunks=chunks,\n",
    "    resolution=res,\n",
    "    bbox=bounding_box,\n",
    "    resampling=\"bilinear\",\n",
    "    groupby=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Rescaling, filling nodata values with np.nan, and adding orbit names, with the helper functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig0_dc = (\n",
    "    post_process_eodc_cube(sig0_dc, items_sig0, bands)\n",
    "    .rename_vars({\"VV\": \"sig0\"})\n",
    "    .assign_coords(orbit=(\"time\", extract_orbit_names(items_sig0)))\n",
    "    .dropna(dim=\"time\", how=\"all\")\n",
    "    .sortby(\"time\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Converting time dimension to a relative orbits dimension:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "__, indices = np.unique(sig0_dc.time, return_index=True)\n",
    "indices.sort()\n",
    "orbit_sig0 = sig0_dc.orbit[indices].data\n",
    "sig0_dc = sig0_dc.groupby(\"time\").mean(skipna=True)\n",
    "sig0_dc = sig0_dc.assign_coords(orbit=(\"time\", orbit_sig0))\n",
    "sig0_dc = sig0_dc.persist()\n",
    "wait(sig0_dc)\n",
    "sig0_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Harmonic Parameters\n",
    "\n",
    "The so-called likelihoods of $P(\\sigma^0|flood)$ and $P(\\sigma^0|nonflood)$ can be calculated from past backscattering information. To be able to this we can model the expected variations in land back scattering based on seasonal changes in vegetation. The procedure is similar to the backscattering routine.\n",
    "\n",
    "Now, instead of just loading the result of this calculation, we'll do it ourselves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_flood_mapper.processing import reduce_to_harmonic_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "We calculate parameters for each relative orbit and then combine them into a single dataset.\n",
    "\n",
    "To do that, we group our $\\sigma^0$ dataset by its `\"orbit\"` variable, and for each group, map the included `reduce_to_harmonic_parameters` function over the spatial chunks of the sub-dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "harm_pars_list = []\n",
    "for orbit, orbit_ds in sig0_dc.groupby(\"orbit\"):\n",
    "    orbit_ds = orbit_ds.chunk({\"time\": -1}).persist()\n",
    "    wait(orbit_ds)\n",
    "    dtimes = orbit_ds[\"time.dayofyear\"].compute()\n",
    "    harm_pars = xr.map_blocks(func=reduce_to_harmonic_parameters,\n",
    "                              obj=orbit_ds[\"sig0\"],\n",
    "                              kwargs={\"dtimes\": dtimes,\n",
    "                                      \"k\": 3,\n",
    "                                      \"x_var_name\": \"longitude\",\n",
    "                                      \"y_var_name\": \"latitude\"}).persist()\n",
    "    harm_pars_list.append((orbit, harm_pars))\n",
    "wait(harm_pars_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5e0cf7",
   "metadata": {},
   "source": [
    "Now we have a dataset of parameters for each orbit, and concatenate them into a single dataset along a new `\"orbit\"` dimension with corresponding coordinates. We also nan-out all pixels where there are fewer than 32 observations underlying the fit (fewer than this tends to give a bad fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a02b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpar_dc = xr.concat([harm_pars[1] for harm_pars in harm_pars_list], dim=\"orbit\")\n",
    "hpar_dc[\"orbit\"] = [harm_pars[0] for harm_pars in harm_pars_list]\n",
    "hpar_dc = hpar_dc.where(hpar_dc.sel(param=\"NOBS\") >= 32).drop_sel(param=\"NOBS\")\n",
    "hpar_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed3a15",
   "metadata": {},
   "source": [
    "A very important step here is to now filter the sigma-nought data down to the period of interest for flood mapping. We then grab the ID of the relative orbit corresponding to each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d74b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig0_dc = sig0_dc.sel(time=slice(*(flood_time_range.split(\"/\"))))\n",
    "orbit_sig0 = sig0_dc.orbit.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "We expand the harmonic parameters dataset along the orbits of sigma nought to be able to calculate the correct land reference backscatter signatures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpar_dc = hpar_dc.to_dataset(dim=\"param\").sel(orbit=orbit_sig0)\n",
    "hpar_dc = hpar_dc.persist()\n",
    "wait(hpar_dc)\n",
    "hpar_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e9406",
   "metadata": {},
   "source": [
    "The rest of the workflow proceeds exactly as in notebook 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Local Incidence Angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = eodc_catalog.search(collections=\"SENTINEL1_MPLIA\", bbox=bounding_box)\n",
    "\n",
    "items_plia = search.item_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = \"MPLIA\"\n",
    "plia_dc = odc_stac.load(\n",
    "    items_plia,\n",
    "    bands=bands,\n",
    "    crs=crs,\n",
    "    chunks=chunks,\n",
    "    resolution=res,\n",
    "    bbox=bounding_box,\n",
    "    groupby=None,\n",
    ")\n",
    "\n",
    "plia_dc = post_process_eodc_cube(plia_dc, items_plia, bands).rename({\"time\": \"orbit\"})\n",
    "plia_dc[\"orbit\"] = extract_orbit_names(items_plia)\n",
    "plia_dc = plia_dc.groupby(\"orbit\").mean(skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plia_dc = plia_dc.sel(orbit=orbit_sig0)\n",
    "plia_dc = plia_dc.persist()\n",
    "wait(plia_dc)\n",
    "plia_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## ESA World Cover from Terrascope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\"\n",
    "wcover_catalog = pystac_client.Client.open(\"https://services.terrascope.be/stac/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = wcover_catalog.search(\n",
    "    collections=\"urn:eop:VITO:ESA_WorldCover_10m_2021_AWS_V2\", bbox=bounding_box\n",
    ")\n",
    "\n",
    "items_wcover = search.item_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcover_dc = (\n",
    "    odc_stac.load(\n",
    "        items_wcover,\n",
    "        crs=crs,\n",
    "        chunks=chunks,\n",
    "        resolution=res,\n",
    "        bbox=bounding_box,\n",
    "    )\n",
    "    .squeeze(\"time\")\n",
    "    .drop_vars(\"time\")\n",
    "    .rename_vars({\"ESA_WORLDCOVER_10M_MAP\": \"wcover\"})\n",
    ")\n",
    "wcover_dc = wcover_dc.persist()\n",
    "wait(wcover_dc)\n",
    "wcover_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Fuse cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_dc = xr.merge([sig0_dc, plia_dc, hpar_dc, wcover_dc])\n",
    "flood_dc = flood_dc.where(flood_dc.wcover != 80)\n",
    "flood_dc = (\n",
    "    flood_dc.reset_index(\"orbit\", drop=True)\n",
    "    .rename({\"orbit\": \"time\"})\n",
    "    .dropna(dim=\"time\", how=\"all\", subset=[\"sig0\"])\n",
    ")\n",
    "flood_dc = flood_dc.persist()\n",
    "wait(flood_dc)\n",
    "flood_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## Likelihoods\n",
    "\n",
    "Now we are ready to calculate the likelihoods of micorwave backscattering given flooding (or non flooding).\n",
    "\n",
    "### Water\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_water_likelihood(dc):\n",
    "    return dc.MPLIA * -0.394181 + -4.142015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_dc[\"wbsc\"] = calc_water_likelihood(flood_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### Land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_expected_backscatter(dc):\n",
    "    w = np.pi * 2 / 365\n",
    "\n",
    "    t = dc.time.dt.dayofyear\n",
    "    wt = w * t\n",
    "\n",
    "    M0 = dc.M0\n",
    "    S1 = dc.S1\n",
    "    S2 = dc.S2\n",
    "    S3 = dc.S3\n",
    "    C1 = dc.C1\n",
    "    C2 = dc.C2\n",
    "    C3 = dc.C3\n",
    "    hm_c1 = (M0 + S1 * np.sin(wt)) + (C1 * np.cos(wt))\n",
    "    hm_c2 = (hm_c1 + S2 * np.sin(2 * wt)) + C2 * np.cos(2 * wt)\n",
    "    hm_c3 = (hm_c2 + S3 * np.sin(3 * wt)) + C3 * np.cos(3 * wt)\n",
    "    return hm_c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_dc[\"hbsc\"] = harmonic_expected_backscatter(flood_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "## Flood mapping\n",
    "\n",
    "Having calculated the likelihoods, we can now move on to calculate the probability of (non-)flooding given a pixel's $\\sigma^0$. These so-called *posteriors* need one more piece of information, as can be seen in the equation above. We need the probability that a pixel is flooded $P(F)$ or not flooded $P(NF)$. Of course, these are the figures we've been trying to find this whole time. We don't actually have them yet, so what can we do? In Bayesian statistics, we can just start with our best guess. These guesses are called our \"priors\", because they are the beliefs we hold *prior* to looking at the data. This subjective prior belief is the foundation Bayesian statistics, and we use the likelihoods we just calculated to update our belief in this particular hypothesis. This updated belief is called the \"posterior\".\n",
    "\n",
    "Let's say that our best estimate for the chance of flooding versus non-flooding of a pixel is 50-50: a coin flip.  We now can also calculate the probability of backscattering $P(\\sigma^0)$, as the weighted average of the water and land likelihoods, ensuring that our posteriors range between 0 to 1.\n",
    "\n",
    "The following code block shows how we calculate the priors which allow use to predict whether it is likely if a land pixel became flooded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_flood_decision(dc):\n",
    "    nf_std = 2.754041\n",
    "    sig0 = dc.sig0\n",
    "    std = dc.STD\n",
    "    wbsc = dc.wbsc\n",
    "    hbsc = dc.hbsc\n",
    "\n",
    "    f_prob = (1.0 / (std * np.sqrt(2 * np.pi))) * np.exp(\n",
    "        -0.5 * (((sig0 - wbsc) / nf_std) ** 2)\n",
    "    )\n",
    "    nf_prob = (1.0 / (nf_std * np.sqrt(2 * np.pi))) * np.exp(\n",
    "        -0.5 * (((sig0 - hbsc) / nf_std) ** 2)\n",
    "    )\n",
    "\n",
    "    evidence = (nf_prob * 0.5) + (f_prob * 0.5)\n",
    "    nf_post_prob = (nf_prob * 0.5) / evidence\n",
    "    f_post_prob = (f_prob * 0.5) / evidence\n",
    "    decision = xr.where(\n",
    "        np.isnan(f_post_prob) | np.isnan(nf_post_prob),\n",
    "        np.nan,\n",
    "        np.greater(f_post_prob, nf_post_prob),\n",
    "    )\n",
    "    return nf_post_prob, f_post_prob, decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_dc[[\"nf_post_prob\", \"f_post_prob\", \"decision\"]] = bayesian_flood_decision(\n",
    "    flood_dc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "## Postprocessing\n",
    "\n",
    "We continue by improving our flood map by filtering out observations that we expect to have low sensitivity to flooding based on a predefined set of criteria.\n",
    "\n",
    "These criteria include:\n",
    "* Masking of Exceeding Incidence Angles\n",
    "* Identification of Conflicting Distributions\n",
    "* Removal of Measurement Outliers\n",
    "* Denial of High Uncertainty on Decision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(dc):\n",
    "    dc = dc * np.logical_and(dc.MPLIA >= 27, dc.MPLIA <= 48)\n",
    "    dc = dc * (dc.hbsc > (dc.wbsc + 0.5 * 2.754041))\n",
    "    land_bsc_lower = dc.hbsc - 3 * dc.STD\n",
    "    land_bsc_upper = dc.hbsc + 3 * dc.STD\n",
    "    water_bsc_upper = dc.wbsc + 3 * 2.754041\n",
    "    mask_land_outliers = np.logical_and(\n",
    "        dc.sig0 > land_bsc_lower, dc.sig0 < land_bsc_upper\n",
    "    )\n",
    "    mask_water_outliers = dc.sig0 < water_bsc_upper\n",
    "    dc = dc * (mask_land_outliers | mask_water_outliers)\n",
    "    return (dc * (dc.f_post_prob > 0.8)).decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_output = post_processing(flood_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "## Removal of Speckles\n",
    "\n",
    "The following step is designed to further improve the clarity of the floodmaps. These filters do not directly relate to prior knowledge on backscattering, but consists of contextual evidence that supports, or oppose, a flood classification. This mainly targets so-called speckles. These speckles are areas of one or a few pixels, and which are likely the result of the diversity of scattering surfaces at a sub-pixel level. In this approach it is argued that small, solitary flood surfaces are unlikely. Hence speckles are removed by applying a smoothing filter which consists of a rolling window median along the x and y-axis simultaneously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_output = (\n",
    "    flood_output.rolling({\"longitude\": 5, \"latitude\": 5}, center=True)\n",
    "    .median(skipna=True)\n",
    "    .persist()\n",
    ")\n",
    "wait(flood_output)\n",
    "flood_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "In the following graphic we superimpose the data on a map and we can move the slider to see which areas become flooded over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_output.hvplot.image(\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    rasterize=True,\n",
    "    geo=True,\n",
    "    tiles=True,\n",
    "    project=True,\n",
    "    cmap=[\"rgba(0, 0, 1, 0.1)\", \"darkred\"],\n",
    "    cticks=[(0, \"non-flood\"), (1, \"flood\")],\n",
    "    frame_height=400,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask-flood-mapper-z0eq0g_n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
